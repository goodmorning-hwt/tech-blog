<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><script src="/tech-blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=tech-blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Diffusion Model 学习笔记1 | 我的技术笔记</title>
<meta name="keywords" content="">
<meta name="description" content="我的 Diffusion Model 学习笔记：逐行拆解代码，搞懂背后原理
大家好！最近我一头扎进了 Diffusion Model 的世界，决定从头开始手撕代码，真正搞懂这个神奇的东西是怎么工作的。最好的学习方法就是对着代码自言自语，一边看一边问，一边想一边答。
下面就是我“手撕”一个基础 Diffusion Model 训练过程的笔记。我把它整理了出来，希望能给同样在学习路上的朋友们一些启发。
Part 1：搭建模型骨架 - UNet2DModel
一切始于模型的定义。在 diffusers 库中，我们可以很方便地初始化一个 U-Net 模型。它是 Diffusion Model 的核心。


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20


# Create a model
model = UNet2DModel(
    sample_size=image_size,      # the target image resolution
    in_channels=3,               # the number of input channels, 3 for RGB images
    out_channels=3,              # the number of output channels
    layers_per_block=2,          # how many ResNet layers to use per UNet block
    block_out_channels=(64, 128, 128, 256),  # More channels -&gt; more parameters
    down_block_types=(
        &#34;DownBlock2D&#34;,           # a regular ResNet downsampling block
        &#34;DownBlock2D&#34;,
        &#34;AttnDownBlock2D&#34;,       # a ResNet downsampling block with spatial self-attention
        &#34;AttnDownBlock2D&#34;,
    ),
    up_block_types=(
        &#34;AttnUpBlock2D&#34;,
        &#34;AttnUpBlock2D&#34;,         # a ResNet upsampling block with spatial self-attention
        &#34;UpBlock2D&#34;,
        &#34;UpBlock2D&#34;,             # a regular ResNet upsampling block
    ),
)


看到这段代码，我问了自己好几个“为什么”。">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/tech-blog/posts/diffussion-note/">
<link crossorigin="anonymous" href="/tech-blog/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/tech-blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/tech-blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/tech-blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/tech-blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/tech-blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/tech-blog/posts/diffussion-note/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/tech-blog/" accesskey="h" title="我的技术笔记 (Alt + H)">我的技术笔记</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/tech-blog/en/" title="English"
                            aria-label="English">En</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Diffusion Model 学习笔记1
    </h1>
    <div class="post-meta"><span title='2025-07-19 20:00:00 +0800 CST'>七月 19, 2025</span>

</div>
  </header> 
  <div class="post-content"><h2 id="我的-diffusion-model-学习笔记逐行拆解代码搞懂背后原理">我的 Diffusion Model 学习笔记：逐行拆解代码，搞懂背后原理<a hidden class="anchor" aria-hidden="true" href="#我的-diffusion-model-学习笔记逐行拆解代码搞懂背后原理">#</a></h2>
<p>大家好！最近我一头扎进了 Diffusion Model 的世界，决定从头开始手撕代码，真正搞懂这个神奇的东西是怎么工作的。最好的学习方法就是对着代码自言自语，一边看一边问，一边想一边答。</p>
<p>下面就是我“手撕”一个基础 Diffusion Model 训练过程的笔记。我把它整理了出来，希望能给同样在学习路上的朋友们一些启发。</p>
<h3 id="part-1搭建模型骨架---unet2dmodel">Part 1：搭建模型骨架 - UNet2DModel<a hidden class="anchor" aria-hidden="true" href="#part-1搭建模型骨架---unet2dmodel">#</a></h3>
<p>一切始于模型的定义。在 <code>diffusers</code> 库中，我们可以很方便地初始化一个 U-Net 模型。它是 Diffusion Model 的核心。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Create a model</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">UNet2DModel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">sample_size</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>      <span class="c1"># the target image resolution</span>
</span></span><span class="line"><span class="cl">    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>               <span class="c1"># the number of input channels, 3 for RGB images</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>              <span class="c1"># the number of output channels</span>
</span></span><span class="line"><span class="cl">    <span class="n">layers_per_block</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>          <span class="c1"># how many ResNet layers to use per UNet block</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_out_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>  <span class="c1"># More channels -&gt; more parameters</span>
</span></span><span class="line"><span class="cl">    <span class="n">down_block_types</span><span class="o">=</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>           <span class="c1"># a regular ResNet downsampling block</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;AttnDownBlock2D&#34;</span><span class="p">,</span>       <span class="c1"># a ResNet downsampling block with spatial self-attention</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;AttnDownBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">up_block_types</span><span class="o">=</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;AttnUpBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;AttnUpBlock2D&#34;</span><span class="p">,</span>         <span class="c1"># a ResNet upsampling block with spatial self-attention</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>             <span class="c1"># a regular ResNet upsampling block</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>看到这段代码，我问了自己好几个“为什么”。</p>
<ul>
<li>
<p><strong><code>sample_size</code></strong>: 这定义了模型处理的图片分辨率。在预处理阶段，所有图片都被调整到了 <code>image_size</code> x <code>image_size</code> 的正方形。这是大多数深度学习模型的常规操作，要求输入尺寸的统一。</p>
</li>
<li>
<p><strong><code>in_channels=3</code> 和 <code>out_channels=3</code></strong>: 这里的逻辑很直接。我们的任务是为一张彩色的 RGB 图片去噪。模型输入的是一张带噪声的 RGB 图片（3个通道），而它的目标是预测出被添加进去的噪声，这个噪声同样作用于 RGB 三个通道，所以输出也是3个通道。</p>
</li>
<li>
<p><strong>U-Net 的核心思想</strong>: U-Net 就像一个 “U” 形沙漏。在 “U” 的左侧（下采样），图片尺寸不断变小，而通道数（可以理解为模型学习到的“特征”数量）不断增加。</p>
<ul>
<li><strong><code>block_out_channels=(64, 128, 128, 256)</code></strong>: 这个元组精确地描述了通道数的变化。输入是3通道，经过第一个 block 后，通道数激增到64。在U-Net的最底部，通道数达到了最多的256。这代表模型在更抽象的层面（图片尺寸小）上提取了更丰富的特征信息。</li>
</ul>
</li>
<li>
<p><strong>Block 的类型与 Attention 机制</strong>:</p>
<ul>
<li><strong><code>down_block_types</code> 和 <code>up_block_types</code></strong>: 这里藏着 U-Net 实现的精髓。你会发现，<code>DownBlock2D</code>（普通下采样块）和 <code>AttnDownBlock2D</code>（带注意力机制的下采样块）被混合使用了。</li>
<li><strong>为什么不全是 Attention？</strong> 因为 Attention 机制，特别是 Self-Attention，非常消耗计算资源。它的作用是捕捉图像中长距离的依赖关系，建立“全局”视野。</li>
<li><strong>策略性地使用 Attention</strong>: 因此，模型的设计者做了一个权衡。在U-Net的浅层，使用普通的卷积块（<code>DownBlock2D</code>）来高效地学习局部特征。当特征图被压缩，信息更密集时（例如通道数从128到256），再引入更昂贵的 Attention 块 (<code>AttnDownBlock2D</code>) 来学习全局信息。这是一种非常聪明的、兼顾效率和效果的设计。</li>
<li><strong><code>up_block_types</code></strong> 正好是 <code>down_block_types</code> 的镜像，它负责将这些抽象的、高维度的特征逐步“解码”，还原成图片尺寸，同时减少通道数，最终输出预测的噪声。</li>
</ul>
</li>
</ul>
<h3 id="part-2训练的核心---training-loop">Part 2：训练的核心 - Training Loop<a hidden class="anchor" aria-hidden="true" href="#part-2训练的核心---training-loop">#</a></h3>
<p>模型搭好了，就要开始“喂”数据训练了。这一部分是整个流程的心脏。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Set the noise scheduler</span>
</span></span><span class="line"><span class="cl"><span class="n">noise_scheduler</span> <span class="o">=</span> <span class="n">DDPMScheduler</span><span class="p">(</span><span class="n">num_train_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">beta_schedule</span><span class="o">=</span><span class="s2">&#34;squaredcos_cap_v2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Training loop</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">4e-4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">clean_images</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&#34;images&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Sample noise to add to the images</span>
</span></span><span class="line"><span class="cl">        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">clean_images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">clean_images</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">bs</span> <span class="o">=</span> <span class="n">clean_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Sample a random timestep for each image</span>
</span></span><span class="line"><span class="cl">        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">num_train_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">clean_images</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Add noise to the clean images according to the noise magnitude at each timestep</span>
</span></span><span class="line"><span class="cl">        <span class="n">noisy_images</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">add_noise</span><span class="p">(</span><span class="n">clean_images</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Get the model prediction</span>
</span></span><span class="line"><span class="cl">        <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Calculate the loss</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Corrected from loss.backward(loss)</span>
</span></span><span class="line"><span class="cl">        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Update the model parameters with the optimizer</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss_last_epoch</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="p">:])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Epoch:</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, loss: </span><span class="si">{</span><span class="n">loss_last_epoch</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们一步一步拆解这个循环：</p>
<ol>
<li>
<p><strong><code>DDPMScheduler</code></strong>: 这是一个“噪声调度器”。Diffusion Model 的核心就是模拟一个从清晰到完全噪声的过程。<code>num_train_timesteps=1000</code> 定义了这个过程总共有1000步。调度器的作用就是根据当前的步数（timestep），精确地计算出应该添加多少噪声。</p>
</li>
<li>
<p><strong><code>optimizer</code></strong>: 我们选择了 <code>AdamW</code>，一个非常经典且强大的优化器。它的任务是根据计算出的损失（loss），来更新模型的所有可学习参数（<code>model.parameters()</code>）。<code>lr=4e-4</code> (即 $4 \times 10^{-4}$) 是学习率，控制每次参数更新的步子大小。</p>
</li>
<li>
<p><strong>循环内部</strong>:</p>
<ul>
<li><strong>获取数据</strong>: <code>clean_images = batch[&quot;images&quot;].to(device)</code> 从数据加载器中取出一批干净的图片，并用 <code>.to(device)</code> 把它们送到 GPU 上加速计算。</li>
<li><strong>生成噪声</strong>: <code>noise = torch.randn(...)</code> 生成与图片尺寸完全相同的随机噪声。</li>
<li><strong>随机选择<code>timestep</code></strong>: 这是训练的关键！<code>timesteps = torch.randint(...)</code> 会从 <code>0</code> 到 <code>999</code> 之间为批次里的每一张图片随机选择一个时间步。这意味着，模型在同一次训练中，会同时学习给“微噪声”图片去噪和给“重噪声”图片去噪。这使得模型非常鲁棒。</li>
<li><strong>加噪</strong>: <code>noisy_images = noise_scheduler.add_noise(...)</code> 调用调度器，根据干净图片、噪声和随机选择的 <code>timestep</code>，生成相应噪声水平的训练样本。</li>
<li><strong>模型预测</strong>: <code>noise_pred = model(...)</code> 把加噪后的图片和它对应的时间步 <code>timestep</code> 一起送入 U-Net 模型。<strong>注意：模型的目标不是预测出原始的清晰图片，而是预测出我们当初添加的 <code>noise</code></strong>。</li>
<li><strong>计算损失</strong>: <code>loss = F.mse_loss(noise_pred, noise)</code> 使用均方误差（Mean Squared Error）来比较模型预测的噪声 <code>noise_pred</code> 和我们真实添加的噪声 <code>noise</code>。因为我们预测的是连续的像素值（噪声），这是一个回归任务，MSE 是非常适合的损失函数。</li>
<li><strong>反向传播与优化</strong>:
<ul>
<li><code>loss.backward()</code>: 计算损失函数关于模型所有参数的梯度。</li>
<li><code>optimizer.step()</code>: 优化器根据刚才计算出的梯度，使用 AdamW 算法来更新模型的权重。</li>
<li><code>optimizer.zero_grad()</code>: 这是至关重要的一步！PyTorch 的梯度是默认累加的。在下一次迭代前，我们必须清空上一轮的梯度，否则梯度会像滚雪球一样越累越大，导致训练出错。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="part-3准备燃料---加载数据集">Part 3：准备“燃料” - 加载数据集<a hidden class="anchor" aria-hidden="true" href="#part-3准备燃料---加载数据集">#</a></h3>
<p>再好的模型也需要数据来训练。这里我们使用了经典的 <code>MNIST</code> 数据集。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">root</span><span class="o">=</span><span class="s2">&#34;mnist/&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>torchvision.datasets.MNIST</code>: PyTorch 自带的计算机视觉库，可以方便地下载和使用标准数据集。</li>
<li><code>root=&quot;mnist/&quot;</code>: 指定数据集下载后存放的目录。</li>
<li><code>train=True</code>: 表明我们加载的是训练集。通常，一个数据集会被分为训练集（Training Set）、验证集（Validation Set）和测试集（Test Set）。训练集用于训练模型，验证集用于调整超参数和评估模型是否过拟合，而测试集则是在模型训练完全结束后，用来评估其最终性能的“期末考试”，它在训练过程中是完全不可见的。</li>
<li><code>transform=torchvision.transforms.ToTensor()</code>: 这是一个转换函数，它会将下载下来的 PIL 图像格式转换为 PyTorch 的 Tensor 格式，方便后续在模型中进行计算。</li>
</ul>
<h3 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h3>
<p>通过这样一番“手撕”，我感觉对 Diffusion Model 的训练过程有了更深刻的理解。它其实就是：</p>
<ol>
<li><strong>一个 U-Net 模型，任务是预测噪声。</strong></li>
<li><strong>一个噪声调度器，负责在0到T步之间精确加噪。</strong></li>
<li><strong>一个训练循环，不断地（随机加噪 -&gt; 预测噪声 -&gt; 计算差距 -&gt; 更新模型），直到模型能精准地预测出任何时间步下的噪声。</strong></li>
</ol>
<p>这个过程虽然听起来简单，但每个细节都蕴含着巧妙的设计。希望我的这篇笔记能帮你拨开迷雾，下次我们再一起探索更有趣的部分！</p>
<hr>
<h3 id="附个人笔记中的勘误与补充">附：个人笔记中的“勘误”与补充<a hidden class="anchor" aria-hidden="true" href="#附个人笔记中的勘误与补充">#</a></h3>
<p>你在梳理自己思路的时候，整体的理解和直觉都非常准确，这很难得！这里只是一些小的技术点，可以让描述更精确：</p>
<ol>
<li>
<p><strong>关于 <code>clean_images.shape</code></strong>: 你在思考时提到 <code>shape</code> 可能是2，但很快意识到它是一个包含多个维度的元组。说得完全正确！一个典型的图像批次张量，其 <code>shape</code> 应该是 <code>(batch_size, channels, height, width)</code>。所以 <code>clean_images.shape[0]</code> 精确地取出了批次大小 <code>batch_size</code>。</p>
</li>
<li>
<p><strong>关于 <code>loss.backward(loss)</code></strong>: 在你的代码片段中，写的是 <code>loss.backward(loss)</code>。对于一个标量（scalar）损失值，直接调用 <code>loss.backward()</code> 就足够了。PyTorch 会自动计算所有参数相对于这个标量的梯度。<code>loss.backward(gradient)</code> 这种带参数的用法通常在更复杂的场景下使用，比如当 <code>loss</code> 是一个向量时，你需要提供一个梯度张量来计算“向量-雅可比积”。在我们的场景下，直接用 <code>loss.backward()</code> 即可。</p>
</li>
<li>
<p><strong>关于数据集划分</strong>: 你提到了训练集和测试集，并思考了验证集的作用。总结得很好！在实际项目中，将数据分为“训练-验证-测试”三部分是标准的做法，它可以有效地防止“数据泄露”（即用测试数据的信息来调整模型），从而得到对模型泛化能力更可靠的评估。</p>
</li>
</ol>
<p>你的笔记非常棒，充满了思考的火花。继续保持！</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/tech-blog/">我的技术笔记</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
